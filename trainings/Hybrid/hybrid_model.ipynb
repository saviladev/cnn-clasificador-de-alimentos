{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":33884,"sourceType":"datasetVersion","datasetId":1864}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import tensorflow as tf\nimport os\nimport numpy as np\n# No necesitaremos ImageDataGenerator para esta forma de cargar los datos\n# from tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.applications import Xception, InceptionV3, ResNet50\nfrom tensorflow.keras.layers import Input, concatenate, Dense, Dropout, GlobalAveragePooling2D\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n\n# 1.1. Verificar GPU (opcional, pero buena práctica en Kaggle Notebooks)\nprint(\"GPU available:\", tf.config.list_physical_devices('GPU'))\nif tf.config.list_physical_devices('GPU'):\n    print(\"You're using a GPU! This will speed up training.\")\nelse:\n    print(\"No GPU found. Training will be slow.\")\n\n# 1.2. Definir las rutas base a tu dataset en Kaggle\n# Basado en la imagen que proporcionaste\nDATASET_ROOT_DIR = '/kaggle/input/food41/' # Ruta base donde está la carpeta food-101\n\n# Rutas a las subcarpetas clave\nIMAGES_DIR = os.path.join(DATASET_ROOT_DIR, 'images')\nMETA_DIR = os.path.join(DATASET_ROOT_DIR, 'meta', 'meta') # Observa la doble carpeta 'meta'\n\n# Rutas a los archivos de split\nTRAIN_FILE = os.path.join(META_DIR, 'train.txt')\nTEST_FILE = os.path.join(META_DIR, 'test.txt')\nCLASSES_FILE = os.path.join(META_DIR, 'classes.txt')\n\nBATCH_SIZE = 32\n\nprint(f\"\\nDataset root directory: {DATASET_ROOT_DIR}\")\nprint(f\"Images directory: {IMAGES_DIR}\")\nprint(f\"Meta directory: {META_DIR}\")\nprint(f\"Train file: {TRAIN_FILE}\")\nprint(f\"Test file: {TEST_FILE}\")\nprint(f\"Classes file: {CLASSES_FILE}\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-07-19T22:17:56.867161Z","iopub.execute_input":"2025-07-19T22:17:56.867397Z","iopub.status.idle":"2025-07-19T22:18:12.119677Z","shell.execute_reply.started":"2025-07-19T22:17:56.867365Z","shell.execute_reply":"2025-07-19T22:18:12.118765Z"}},"outputs":[{"name":"stderr","text":"2025-07-19 22:17:58.545319: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1752963478.769859      36 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1752963478.837536      36 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"GPU available: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU'), PhysicalDevice(name='/physical_device:GPU:1', device_type='GPU')]\nYou're using a GPU! This will speed up training.\n\nDataset root directory: /kaggle/input/food41/\nImages directory: /kaggle/input/food41/images\nMeta directory: /kaggle/input/food41/meta/meta\nTrain file: /kaggle/input/food41/meta/meta/train.txt\nTest file: /kaggle/input/food41/meta/meta/test.txt\nClasses file: /kaggle/input/food41/meta/meta/classes.txt\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# 2.1. Cargar nombres de clases y crear mapeo a etiquetas numéricas\nclass_names = []\nwith open(CLASSES_FILE, 'r') as f:\n    for line in f:\n        class_names.append(line.strip()) # Eliminar espacios en blanco y saltos de línea\n\nclass_to_idx = {name: i for i, name in enumerate(class_names)}\nNUM_CLASSES = len(class_names)\nprint(f\"\\nDetected classes ({NUM_CLASSES}): {class_names}\")\n\n# 2.2. Función para leer los archivos train.txt/test.txt y obtener rutas y etiquetas\ndef get_image_paths_and_labels_from_txt(file_path, base_images_dir, class_to_idx):\n    image_paths = []\n    labels = []\n    with open(file_path, 'r') as f:\n        for line in f:\n            relative_path = line.strip() # Ej: 'apple_pie/1000.jpg'\n            \n            # Extraer el nombre de la clase de la ruta relativa\n            class_name = relative_path.split('/')[0]\n            label = class_to_idx[class_name] # Mapear a entero\n\n            # Construir la ruta completa a la imagen\n            full_path = os.path.join(base_images_dir, relative_path + '.jpg') # Asume .jpg, Food-101 usa .jpg\n            \n            image_paths.append(full_path)\n            labels.append(label)\n    return image_paths, labels\n\n# Obtener rutas y etiquetas para entrenamiento y prueba\nprint(\"\\nLoading training image paths and labels...\")\ntrain_image_paths, train_labels = get_image_paths_and_labels_from_txt(TRAIN_FILE, IMAGES_DIR, class_to_idx)\nprint(f\"Training images found: {len(train_image_paths)}\")\n\nprint(\"Loading test image paths and labels...\")\ntest_image_paths, test_labels = get_image_paths_and_labels_from_txt(TEST_FILE, IMAGES_DIR, class_to_idx)\nprint(f\"Test images found: {len(test_image_paths)}\")\n\n# 2.3. Función de carga y preprocesamiento para tf.data\n# Esta función leerá una imagen y la redimensionará a ambos tamaños requeridos.\ndef load_and_preprocess(image_path, label):\n    # Cargar la imagen\n    image = tf.io.read_file(image_path)\n    image = tf.image.decode_jpeg(image, channels=3) # Decodifica JPEG a 3 canales (RGB)\n    \n    # Redimensionar y normalizar la imagen para ambas entradas\n    image_299 = tf.image.resize(image, (299, 299)) / 255.0\n    image_224 = tf.image.resize(image, (224, 224)) / 255.0\n    \n    # Devuelve un diccionario para las múltiples entradas del modelo y la etiqueta\n    return {'input_299x299': image_299, 'input_224x224': image_224}, label\n\n# 2.4. Crear los objetos tf.data.Dataset\n# Crea datasets a partir de las rutas y etiquetas\ntrain_ds = tf.data.Dataset.from_tensor_slices((tf.constant(train_image_paths), tf.constant(train_labels)))\ntest_ds = tf.data.Dataset.from_tensor_slices((tf.constant(test_image_paths), tf.constant(test_labels)))\n\n# Configurar el pipeline de datos para eficiencia\n# `shuffle`: Mezcla los datos para el entrenamiento (solo en train_ds).\n# `map`: Aplica la función de preprocesamiento a cada elemento. `num_parallel_calls` acelera esto.\n# `batch`: Agrupa los elementos en batches.\n# `prefetch`: Superpone el preprocesamiento de datos con el entrenamiento del modelo.\ntrain_ds = train_ds.shuffle(buffer_size=len(train_image_paths)).map(load_and_preprocess, num_parallel_calls=tf.data.AUTOTUNE).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\ntest_ds = test_ds.map(load_and_preprocess, num_parallel_calls=tf.data.AUTOTUNE).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n\nprint(\"\\ntf.data.Dataset for training and testing are ready.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-19T22:18:20.469074Z","iopub.execute_input":"2025-07-19T22:18:20.469349Z","iopub.status.idle":"2025-07-19T22:18:21.039984Z","shell.execute_reply.started":"2025-07-19T22:18:20.469328Z","shell.execute_reply":"2025-07-19T22:18:21.039202Z"}},"outputs":[{"name":"stdout","text":"\nDetected classes (101): ['apple_pie', 'baby_back_ribs', 'baklava', 'beef_carpaccio', 'beef_tartare', 'beet_salad', 'beignets', 'bibimbap', 'bread_pudding', 'breakfast_burrito', 'bruschetta', 'caesar_salad', 'cannoli', 'caprese_salad', 'carrot_cake', 'ceviche', 'cheesecake', 'cheese_plate', 'chicken_curry', 'chicken_quesadilla', 'chicken_wings', 'chocolate_cake', 'chocolate_mousse', 'churros', 'clam_chowder', 'club_sandwich', 'crab_cakes', 'creme_brulee', 'croque_madame', 'cup_cakes', 'deviled_eggs', 'donuts', 'dumplings', 'edamame', 'eggs_benedict', 'escargots', 'falafel', 'filet_mignon', 'fish_and_chips', 'foie_gras', 'french_fries', 'french_onion_soup', 'french_toast', 'fried_calamari', 'fried_rice', 'frozen_yogurt', 'garlic_bread', 'gnocchi', 'greek_salad', 'grilled_cheese_sandwich', 'grilled_salmon', 'guacamole', 'gyoza', 'hamburger', 'hot_and_sour_soup', 'hot_dog', 'huevos_rancheros', 'hummus', 'ice_cream', 'lasagna', 'lobster_bisque', 'lobster_roll_sandwich', 'macaroni_and_cheese', 'macarons', 'miso_soup', 'mussels', 'nachos', 'omelette', 'onion_rings', 'oysters', 'pad_thai', 'paella', 'pancakes', 'panna_cotta', 'peking_duck', 'pho', 'pizza', 'pork_chop', 'poutine', 'prime_rib', 'pulled_pork_sandwich', 'ramen', 'ravioli', 'red_velvet_cake', 'risotto', 'samosa', 'sashimi', 'scallops', 'seaweed_salad', 'shrimp_and_grits', 'spaghetti_bolognese', 'spaghetti_carbonara', 'spring_rolls', 'steak', 'strawberry_shortcake', 'sushi', 'tacos', 'takoyaki', 'tiramisu', 'tuna_tartare', 'waffles']\n\nLoading training image paths and labels...\nTraining images found: 75750\nLoading test image paths and labels...\nTest images found: 25250\n\ntf.data.Dataset for training and testing are ready.\n","output_type":"stream"},{"name":"stderr","text":"I0000 00:00:1752963500.893891      36 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13942 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\nI0000 00:00:1752963500.894570      36 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 13942 MB memory:  -> device: 1, name: Tesla T4, pci bus id: 0000:00:05.0, compute capability: 7.5\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# 3.1. Definir las capas de entrada para cada tamaño de imagen\ninput_299 = Input(shape=(299, 299, 3), name='input_299x299')\ninput_224 = Input(shape=(224, 224, 3), name='input_224x224')\n\n# 3.2. Cargar modelos base pre-entrenados (con pesos de ImageNet) y congelar sus capas\n# Usaremos `pooling='avg'` para añadir una capa GlobalAveragePooling2D y aplanar las características.\n\n# Xception\nprint(\"\\nConfiguring Xception base...\")\nbase_xception = Xception(weights='imagenet', include_top=False, input_tensor=input_299, pooling='avg')\nfor layer in base_xception.layers:\n    layer.trainable = False # Congelar las capas de la base pre-entrenada\nxception_output = base_xception.output\n\n# InceptionV3\nprint(\"Configuring InceptionV3 base...\")\nbase_inception = InceptionV3(weights='imagenet', include_top=False, input_tensor=input_299, pooling='avg')\nfor layer in base_inception.layers:\n    layer.trainable = False # Congelar las capas\ninception_output = base_inception.output\n\n# ResNet50\nprint(\"Configuring ResNet50 base...\")\nbase_resnet = ResNet50(weights='imagenet', include_top=False, input_tensor=input_224, pooling='avg')\nfor layer in base_resnet.layers:\n    layer.trainable = False # Congelar las capas\nresnet_output = base_resnet.output\n\nprint(\"\\nAll base models configured and frozen.\")\n\n# 3.3. Concatenar las características de los tres modelos\nmerged_features = concatenate([xception_output, inception_output, resnet_output])\n\n# 3.4. Añadir una nueva cabeza de clasificación (capas densas)\nx = Dense(512, activation='relu')(merged_features)\nx = Dropout(0.5)(x) # Capa de dropout para regularización\noutput_layer = Dense(NUM_CLASSES, activation='softmax')(x) # Capa de salida con activación softmax\n\n# 3.5. Crear el modelo híbrido final\nhybrid_model = Model(inputs=[input_299, input_224], outputs=output_layer)\n\n# 3.6. Compilar el modelo\nhybrid_model.compile(optimizer='adam',\n                     loss='sparse_categorical_crossentropy', # Usar para etiquetas de enteros\n                     metrics=['accuracy'])\n\nprint(\"\\nSummary del Modelo Híbrido (solo la nueva cabeza se entrenará):\")\nhybrid_model.summary()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-19T22:18:25.035754Z","iopub.execute_input":"2025-07-19T22:18:25.036031Z","iopub.status.idle":"2025-07-19T22:18:32.461091Z"}},"outputs":[{"name":"stdout","text":"\nConfiguring Xception base...\nDownloading data from https://storage.googleapis.com/tensorflow/keras-applications/xception/xception_weights_tf_dim_ordering_tf_kernels_notop.h5\n\u001b[1m83683744/83683744\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\nConfiguring InceptionV3 base...\nDownloading data from https://storage.googleapis.com/tensorflow/keras-applications/inception_v3/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5\n\u001b[1m87910968/87910968\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 0us/step\nConfiguring ResNet50 base...\nDownloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n\u001b[1m94765736/94765736\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n\nAll base models configured and frozen.\n\nSummary del Modelo Híbrido (solo la nueva cabeza se entrenará):\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"\u001b[1mModel: \"functional\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n</pre>\n"},"metadata":{}}],"execution_count":null},{"cell_type":"code","source":"# 4.1. Crear directorio para guardar checkpoints en tu Kaggle /kaggle/working/\ncheckpoint_dir = '/kaggle/working/food101_hybrid_checkpoints'\nos.makedirs(checkpoint_dir, exist_ok=True)\nprint(f\"\\nCheckpoint directory created at: {checkpoint_dir}\")\n\n# 4.2. Definir el callback ModelCheckpoint\ncheckpoint_filepath = os.path.join(checkpoint_dir, 'best_hybrid_food101_model.h5')\nmodel_checkpoint_callback = ModelCheckpoint(\n    filepath=checkpoint_filepath,\n    monitor='val_accuracy',\n    mode='max',\n    save_best_only=True,\n    verbose=1\n)\n\n# 4.3. Definir el callback EarlyStopping\nearly_stopping_callback = EarlyStopping(\n    monitor='val_accuracy',\n    patience=3,\n    mode='max',\n    restore_best_weights=True,\n    verbose=1\n)\n\n# 4.4. Agrupar los callbacks en una lista\ncallbacks_list = [model_checkpoint_callback, early_stopping_callback]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-19T22:18:45.491856Z","iopub.execute_input":"2025-07-19T22:18:45.492425Z","iopub.status.idle":"2025-07-19T22:18:45.497982Z","shell.execute_reply.started":"2025-07-19T22:18:45.492401Z","shell.execute_reply":"2025-07-19T22:18:45.497232Z"}},"outputs":[{"name":"stdout","text":"\nCheckpoint directory created at: /kaggle/working/food101_hybrid_checkpoints\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"# 5.1. Definir el número máximo de épocas\nEPOCHS = 15\n\n# 5.2. Iniciar el entrenamiento\nprint(\"\\nStarting the training of the Hybrid Model's classification head...\")\nhistory = hybrid_model.fit(\n    train_ds,              # Ahora pasamos directamente el tf.data.Dataset\n    epochs=EPOCHS,\n    validation_data=test_ds, # Y aquí el tf.data.Dataset de validación/prueba\n    callbacks=callbacks_list\n)\n\nprint(\"\\nHybrid Model training completed.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-19T22:18:57.217535Z","iopub.execute_input":"2025-07-19T22:18:57.217855Z","iopub.status.idle":"2025-07-20T04:54:51.869961Z","shell.execute_reply.started":"2025-07-19T22:18:57.217832Z","shell.execute_reply":"2025-07-20T04:54:51.869103Z"}},"outputs":[{"name":"stdout","text":"\nStarting the training of the Hybrid Model's classification head...\nEpoch 1/15\n","output_type":"stream"},{"name":"stderr","text":"WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nI0000 00:00:1752963563.817687      97 service.cc:148] XLA service 0x7e006c0da4b0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\nI0000 00:00:1752963563.818501      97 service.cc:156]   StreamExecutor device (0): Tesla T4, Compute Capability 7.5\nI0000 00:00:1752963563.818520      97 service.cc:156]   StreamExecutor device (1): Tesla T4, Compute Capability 7.5\nI0000 00:00:1752963567.891408      97 cuda_dnn.cc:529] Loaded cuDNN version 90300\n2025-07-19 22:19:33.855202: E external/local_xla/xla/service/slow_operation_alarm.cc:65] Trying algorithm eng3{k11=0} for conv (f32[32,128,147,147]{3,2,1,0}, u8[0]{0}) custom-call(f32[32,128,147,147]{3,2,1,0}, f32[128,128,1,1]{3,2,1,0}), window={size=1x1}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convForward\", backend_config={\"cudnn_conv_backend_config\":{\"activation_mode\":\"kNone\",\"conv_result_scale\":1,\"leakyrelu_alpha\":0,\"side_input_scale\":0},\"force_earliest_schedule\":false,\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[]} is taking a while...\n2025-07-19 22:19:34.075286: E external/local_xla/xla/service/slow_operation_alarm.cc:133] The operation took 1.22026206s\nTrying algorithm eng3{k11=0} for conv (f32[32,128,147,147]{3,2,1,0}, u8[0]{0}) custom-call(f32[32,128,147,147]{3,2,1,0}, f32[128,128,1,1]{3,2,1,0}), window={size=1x1}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convForward\", backend_config={\"cudnn_conv_backend_config\":{\"activation_mode\":\"kNone\",\"conv_result_scale\":1,\"leakyrelu_alpha\":0,\"side_input_scale\":0},\"force_earliest_schedule\":false,\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[]} is taking a while...\nI0000 00:00:1752963595.049349      97 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m2368/2368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 503ms/step - accuracy: 0.2018 - loss: 3.3954\nEpoch 1: val_accuracy improved from -inf to 0.52123, saving model to /kaggle/working/food101_hybrid_checkpoints/best_hybrid_food101_model.h5\n\u001b[1m2368/2368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1671s\u001b[0m 681ms/step - accuracy: 0.2019 - loss: 3.3952 - val_accuracy: 0.5212 - val_loss: 1.8932\nEpoch 2/15\n\u001b[1m2368/2368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 499ms/step - accuracy: 0.3607 - loss: 2.5169\nEpoch 2: val_accuracy improved from 0.52123 to 0.56012, saving model to /kaggle/working/food101_hybrid_checkpoints/best_hybrid_food101_model.h5\n\u001b[1m2368/2368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1581s\u001b[0m 668ms/step - accuracy: 0.3607 - loss: 2.5168 - val_accuracy: 0.5601 - val_loss: 1.7708\nEpoch 3/15\n\u001b[1m2368/2368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 498ms/step - accuracy: 0.3850 - loss: 2.3834\nEpoch 3: val_accuracy improved from 0.56012 to 0.56851, saving model to /kaggle/working/food101_hybrid_checkpoints/best_hybrid_food101_model.h5\n\u001b[1m2368/2368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1578s\u001b[0m 666ms/step - accuracy: 0.3850 - loss: 2.3834 - val_accuracy: 0.5685 - val_loss: 1.6361\nEpoch 4/15\n\u001b[1m2368/2368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 499ms/step - accuracy: 0.4057 - loss: 2.3181\nEpoch 4: val_accuracy improved from 0.56851 to 0.57865, saving model to /kaggle/working/food101_hybrid_checkpoints/best_hybrid_food101_model.h5\n\u001b[1m2368/2368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1578s\u001b[0m 666ms/step - accuracy: 0.4057 - loss: 2.3181 - val_accuracy: 0.5787 - val_loss: 1.5887\nEpoch 5/15\n\u001b[1m2368/2368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 499ms/step - accuracy: 0.4173 - loss: 2.2525\nEpoch 5: val_accuracy improved from 0.57865 to 0.58875, saving model to /kaggle/working/food101_hybrid_checkpoints/best_hybrid_food101_model.h5\n\u001b[1m2368/2368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1579s\u001b[0m 667ms/step - accuracy: 0.4173 - loss: 2.2525 - val_accuracy: 0.5888 - val_loss: 1.5546\nEpoch 6/15\n\u001b[1m2368/2368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 499ms/step - accuracy: 0.4281 - loss: 2.2047\nEpoch 6: val_accuracy did not improve from 0.58875\n\u001b[1m2368/2368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1577s\u001b[0m 666ms/step - accuracy: 0.4281 - loss: 2.2047 - val_accuracy: 0.5879 - val_loss: 1.5734\nEpoch 7/15\n\u001b[1m2368/2368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 499ms/step - accuracy: 0.4335 - loss: 2.1801\nEpoch 7: val_accuracy improved from 0.58875 to 0.59505, saving model to /kaggle/working/food101_hybrid_checkpoints/best_hybrid_food101_model.h5\n\u001b[1m2368/2368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1580s\u001b[0m 667ms/step - accuracy: 0.4335 - loss: 2.1801 - val_accuracy: 0.5950 - val_loss: 1.5635\nEpoch 8/15\n\u001b[1m2368/2368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 499ms/step - accuracy: 0.4451 - loss: 2.1344\nEpoch 8: val_accuracy did not improve from 0.59505\n\u001b[1m2368/2368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1576s\u001b[0m 666ms/step - accuracy: 0.4451 - loss: 2.1344 - val_accuracy: 0.5941 - val_loss: 1.5396\nEpoch 9/15\n\u001b[1m2368/2368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 498ms/step - accuracy: 0.4484 - loss: 2.1067\nEpoch 9: val_accuracy did not improve from 0.59505\n\u001b[1m2368/2368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1576s\u001b[0m 666ms/step - accuracy: 0.4483 - loss: 2.1067 - val_accuracy: 0.5937 - val_loss: 1.5602\nEpoch 10/15\n\u001b[1m2368/2368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 499ms/step - accuracy: 0.4520 - loss: 2.0897\nEpoch 10: val_accuracy improved from 0.59505 to 0.59861, saving model to /kaggle/working/food101_hybrid_checkpoints/best_hybrid_food101_model.h5\n\u001b[1m2368/2368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1580s\u001b[0m 667ms/step - accuracy: 0.4520 - loss: 2.0897 - val_accuracy: 0.5986 - val_loss: 1.4888\nEpoch 11/15\n\u001b[1m2368/2368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 499ms/step - accuracy: 0.4574 - loss: 2.0666\nEpoch 11: val_accuracy improved from 0.59861 to 0.60931, saving model to /kaggle/working/food101_hybrid_checkpoints/best_hybrid_food101_model.h5\n\u001b[1m2368/2368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1578s\u001b[0m 666ms/step - accuracy: 0.4574 - loss: 2.0666 - val_accuracy: 0.6093 - val_loss: 1.4724\nEpoch 12/15\n\u001b[1m2368/2368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 498ms/step - accuracy: 0.4630 - loss: 2.0358\nEpoch 12: val_accuracy improved from 0.60931 to 0.61869, saving model to /kaggle/working/food101_hybrid_checkpoints/best_hybrid_food101_model.h5\n\u001b[1m2368/2368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1577s\u001b[0m 666ms/step - accuracy: 0.4630 - loss: 2.0358 - val_accuracy: 0.6187 - val_loss: 1.4253\nEpoch 13/15\n\u001b[1m2368/2368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 498ms/step - accuracy: 0.4667 - loss: 2.0194\nEpoch 13: val_accuracy did not improve from 0.61869\n\u001b[1m2368/2368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1575s\u001b[0m 665ms/step - accuracy: 0.4667 - loss: 2.0195 - val_accuracy: 0.6085 - val_loss: 1.4466\nEpoch 14/15\n\u001b[1m2368/2368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 498ms/step - accuracy: 0.4662 - loss: 2.0255\nEpoch 14: val_accuracy did not improve from 0.61869\n\u001b[1m2368/2368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1574s\u001b[0m 665ms/step - accuracy: 0.4662 - loss: 2.0255 - val_accuracy: 0.6100 - val_loss: 1.4776\nEpoch 15/15\n\u001b[1m2368/2368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 498ms/step - accuracy: 0.4690 - loss: 2.0033\nEpoch 15: val_accuracy improved from 0.61869 to 0.62012, saving model to /kaggle/working/food101_hybrid_checkpoints/best_hybrid_food101_model.h5\n\u001b[1m2368/2368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1576s\u001b[0m 665ms/step - accuracy: 0.4690 - loss: 2.0033 - val_accuracy: 0.6201 - val_loss: 1.4293\nRestoring model weights from the end of the best epoch: 15.\n\nHybrid Model training completed.\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"# --- 6. Guardar el Modelo Final ---\n\n# Define la ruta para guardar el modelo final\n# Lo guardaremos en el mismo directorio persistente de Kaggle\nfinal_model_save_path = '/kaggle/working/final_hybrid_food101_model.h5'\n\n# Guarda el modelo. Dado que EarlyStopping.restore_best_weights=True,\n# el `hybrid_model` ya contiene los pesos del mejor checkpoint.\nprint(f\"\\nSaving the final trained hybrid model to: {final_model_save_path}\")\nhybrid_model.save(final_model_save_path)\nprint(\"Final model saved successfully!\")\n\n# Puedes verificar que el archivo existe\nif os.path.exists(final_model_save_path):\n    print(f\"Model file confirmed at: {final_model_save_path}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-20T04:57:23.215451Z","iopub.execute_input":"2025-07-20T04:57:23.215756Z","iopub.status.idle":"2025-07-20T04:57:24.804367Z","shell.execute_reply.started":"2025-07-20T04:57:23.215733Z","shell.execute_reply":"2025-07-20T04:57:24.803525Z"}},"outputs":[{"name":"stdout","text":"\nSaving the final trained hybrid model to: /kaggle/working/final_hybrid_food101_model.h5\nFinal model saved successfully!\nModel file confirmed at: /kaggle/working/final_hybrid_food101_model.h5\n","output_type":"stream"}],"execution_count":7}]}